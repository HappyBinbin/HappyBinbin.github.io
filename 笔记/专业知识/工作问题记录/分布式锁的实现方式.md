## Reference
- https://chai2010.cn/advanced-go-programming-book/ch6-cloud/ch6-02-lock.html

## 背景

最近在工作中遇到了需要分布式锁的情况，发现业界有很多中实现方式，针对不同场景有不同的应对方法。本文会介绍一下实际使用的方式，以及其他的方式实现代码；

## 实现思路

### 单机锁

#### 进程锁

>  通过自带的mutex实现[[同步原语和锁]]

``` go
// 简单锁
// ... 省略之前部分
var wg sync.WaitGroup
var l sync.Mutex
for i := 0; i < 1000; i++ {
    wg.Add(1)
    go func() {
        defer wg.Done()
        l.Lock()
        counter++
        l.Unlock()
    }()
}

wg.Wait()
println(counter)
// ... 省略之后部分

```

#### trylock 锁

> 通过 chan 实现[[channel]]，本质是队列；加锁失败了不会阻塞

``` go
// trylock 锁
package main

import (
    "sync"
)

// Lock try lock
type Lock struct {
    c chan struct{}
}

// NewLock generate a try lock
func NewLock() Lock {
    var l Lock
    l.c = make(chan struct{}, 1)
    l.c <- struct{}{}
    return l
}

// Lock try lock, return lock result
func (l Lock) Lock() bool {
    lockResult := false
    select {
    case <-l.c:
        lockResult = true
    default:
    }
    return lockResult
}

// Unlock , Unlock the try lock
func (l Lock) Unlock() {
    l.c <- struct{}{}
}

var counter int

func main() {
    var l = NewLock()
    var wg sync.WaitGroup
    for i := 0; i < 10; i++ {
        wg.Add(1)
        go func() {
            defer wg.Done()
            if !l.Lock() {
                // log error
                println("lock failed")
                return
            }
            counter++
            println("current counter", counter)
            l.Unlock()
        }()
    }
    wg.Wait()
}
```

#### 自旋锁
利用cas到无锁特性，增强并发

``` go
package main

import (
	"fmt"
	"sync"
	"sync/atomic"
	"time"
)

type CASLock struct {
	state int32 // 0: unlocked, 1: locked
}

func (l *CASLock) Lock() {
	for {
		// 尝试将 state 从 0（未锁定）改为 1（锁定）
		if atomic.CompareAndSwapInt32(&l.state, 0, 1) {
			return
		}
		// 避免忙等待，短暂休眠以降低 CPU 占用
		time.Sleep(time.Nanosecond)
	}
}

func (l *CASLock) Unlock() {
	atomic.StoreInt32(&l.state, 0) // 原子性地将 state 置为 0
}

func main() {
	var lock CASLock
	counter := 0
	var wg sync.WaitGroup

	// 启动 1000 个 goroutine 并发递增计数器
	for i := 0; i < 1000; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			lock.Lock()
			counter++
			lock.Unlock()
		}()
	}

	wg.Wait()
	fmt.Println("Final counter value:", counter) // 预期输出 1000
}

```


### 分布式锁

#### 简单的setnx

``` go
package main

import (
    "fmt"
    "sync"
    "time"

    "github.com/go-redis/redis"
)

func incr() {
    client := redis.NewClient(&redis.Options{
        Addr:     "localhost:6379",
        Password: "", // no password set
        DB:       0,  // use default DB
    })

    var lockKey = "counter_lock"
    var counterKey = "counter"

    // lock
    resp := client.SetNX(lockKey, 1, time.Second*5)
    lockSuccess, err := resp.Result()

    if err != nil || !lockSuccess {
        fmt.Println(err, "lock result:", lockSuccess)
        return
    }

    // counter ++
    getResp := client.Get(counterKey)
    cntValue, err := getResp.Int64()
    if err == nil || err == redis.Nil {
        cntValue++
        resp := client.Set(counterKey, cntValue, 0)
        _, err := resp.Result()
        if err != nil {
            // log err
            println("set value error!")
        }
    }
    println("current counter is", cntValue)

    delResp := client.Del(lockKey)
    unlockSuccess, err := delResp.Result()
    if err == nil && unlockSuccess > 0 {
        println("unlock success!")
    } else {
        println("unlock failed", err)
    }
}

func main() {
    var wg sync.WaitGroup
    for i := 0; i < 10; i++ {
        wg.Add(1)
        go func() {
            defer wg.Done()
            incr()
        }()
    }
    wg.Wait()
}

```

#### lua 脚本

下面的例子展示了如何通过lua脚本，实现redis的分布式锁

``` go
package Redis  
  
import (  
   "context"  
   "log"   
   "math/rand"   
   "strconv"   
   "sync/atomic"   
   "time"  
   
   "github.com/go-redis/redis"   
   "github.com/zeromicro/go-zero/core/stringx"
)  
  
const (  
   randomLen       = 16  
   tolerance       = 500 // milliseconds  
   millisPerSecond = 1000  
  
   acquireLuaScript = `if redis.call("GET", KEYS[1]) == ARGV[1] then  
    redis.call("SET", KEYS[1], ARGV[1], "PX", ARGV[2])    return "OK"else  
    return redis.call("SET", KEYS[1], ARGV[1], "NX", "PX", ARGV[2])end`  
   releaseLuaScript = `if redis.call("GET", KEYS[1]) == ARGV[1] then  
    return redis.call("DEL", KEYS[1])else  
    return 0end`  
  
   retryDelay = time.Millisecond  
)  
  
var acquireScript = redis.NewScript(acquireLuaScript)  
var releaseScript = redis.NewScript(releaseLuaScript)  
  
// A RedisLock is a redis lock.type RedisLock struct {  
   redisCli redis.Client  
   seconds  uint32  
   key      string  
   id       string  
}  
  
// init  
// @description: 初始化随机数种子  
func init() {  
   rand.Seed(time.Now().UnixNano())  
}  
  
// NewRedisLock returns a RedisLock.  
func NewRedisLock(cli redis.Client, key string) *RedisLock {  
   return &RedisLock{  
      redisCli: cli,  
      key:      key,  
      id:       stringx.Randn(randomLen),  
   }  
}  
  
// Acquire acquires the lock.func (rl *RedisLock) 
Acquire(ctx context.Context) (bool, error) {  
   return rl.AcquireCtx(ctx)  
}  
  
// AcquireCtx acquires the lock with the given ctx.func (rl *RedisLock) 
AcquireCtx(ctx context.Context) (bool, error) {  
   seconds := atomic.LoadUint32(&rl.seconds)  
   ret, err := acquireScript.Exists(rl.redisCli.WithContext(ctx)).Result()  
   if err != nil {  
      return false, err  
   }  
   // 如果申请锁脚本不存在，加载申请锁脚本  
   if !ret[0] {  
      _, err = acquireScript.Load(rl.redisCli.WithContext(ctx)).Result()  
      if err != nil {  
         return false, err  
      }  
   }  
  
   resp, err := acquireScript.EvalSha(rl.redisCli.WithContext(ctx), []string{rl.key}, []string{  
      rl.id, strconv.Itoa(int(seconds)*millisPerSecond + tolerance),  
   }).Result()  
   if err == redis.Nil {  
      return false, nil  
   } else if err != nil {  
      log.Printf("Error on acquiring lock for %s, %s", rl.key, err.Error())  
      return false, err  
   } else if resp == nil {  
      return false, nil  
   }  
  
   reply, ok := resp.(string)  
   if ok && reply == "OK" {  
      return true, nil  
   }  
  
   log.Printf("Unknown reply when acquiring lock for %s: %v", rl.key, resp)  
   return false, nil  
}  
  
// Release releases the lock.func (rl *RedisLock) Release(ctx context.Context) (bool, error) {  
   return rl.ReleaseCtx(ctx)  
}  
  
// ReleaseCtx releases the lock with the given ctx.func (rl *RedisLock) 
ReleaseCtx(ctx context.Context) (bool, error) {  
   ret, err := releaseScript.Exists(rl.redisCli.WithContext(ctx)).Result()  
   if err != nil {  
      return false, err  
   }  
   // 如果释放锁脚本不存在，加载释放锁脚本  
   if !ret[0] {  
      _, err = releaseScript.Load(rl.redisCli.WithContext(ctx)).Result()  
      if err != nil {  
         return false, err  
      }  
   }  
  
   resp, err := releaseScript.EvalSha(rl.redisCli.WithContext(ctx), []string{rl.key}, []string{rl.id}).Result()  
   if err != nil {  
      log.Printf("Error on releasing lock for %s, %s", rl.key, err.Error())  
      return false, err  
   }  
  
   reply, ok := resp.(int64)  
   if !ok {  
      return false, nil  
   }  
  
   return reply == 1, nil  
}  
  
// SetExpire sets the expiration.
func (rl *RedisLock) SetExpire(seconds int) {  
   atomic.StoreUint32(&rl.seconds, uint32(seconds))  
}  
  
// TryAcquireCtx  
// @description: 重复尝试获取redis锁，直到成功获取锁或者到达超时时间  
// @param ctx  
// @param timeout 超时时间  
// @return bool  
func (rl *RedisLock) TryAcquireCtx(ctx context.Context, timeout time.Duration) bool {  
   timeoutCtx, cancel := context.WithTimeout(ctx, timeout)  
   defer cancel()  
  
   for {  
      if ok, _ := rl.AcquireCtx(ctx); ok {  
         return ok  
      }  
      select {  
      case <-timeoutCtx.Done():  
         return false  
      case <-time.After(retryDelay):  
         // try again  
      }  
   }  
}  
  
// TryReleaseCtx  
// @description: 重复尝试释放redis锁，直到成功释放锁或者到达超时时间  
// @param ctx  
// @param timeout  
// @return bool  
func (rl *RedisLock) TryReleaseCtx(ctx context.Context, timeout time.Duration) bool {  
  
   timeoutCtx, cancel := context.WithTimeout(ctx, timeout)  
   defer cancel()  
  
   for {  
      if ok, _ := rl.ReleaseCtx(timeoutCtx); ok {  
         return ok  
      }  
      select {  
      case <-timeoutCtx.Done():  
         return false  
      case <-time.After(retryDelay):  
         // try again  
      }  
   }  
}
```

### **Zookeeper 分布式锁**
#### **1. 实现原理**
• **临时顺序节点**：客户端在 Zookeeper 的指定路径下创建临时顺序节点（如 `/locks/lock-00000001`），节点名称包含递增序号。
• **锁获取逻辑**：
  1. 客户端检查自己创建的节点是否为当前最小序号节点。若是，则获取锁。
  2. 若非最小节点，则监听比自己序号小的前一个节点的删除事件。
  3. 当前一个节点被删除（锁释放或客户端崩溃）时，客户端重新检查是否成为最小节点，尝试获取锁。
• **锁释放**：客户端完成操作后主动删除节点，或因异常退出时临时节点自动删除，触发监听机制。

#### **2. 优缺点**
• **优点**：
  • **强一致性**：基于 ZAB 协议保证数据一致性，所有节点状态同步。
  • **自动释放**：临时节点特性避免锁泄漏，无需额外清理。
  • **高可靠性**：集群容错性强，适合金融等高一致性场景。
• **缺点**：
  • **性能瓶颈**：高并发时节点创建和监听操作频繁，延迟较高。
  • **羊群效应**：锁释放时所有等待客户端被唤醒，可能引发竞争风暴。
  • **运维成本**：需维护 Zookeeper 集群的高可用性。

---

### **Etcd 分布式锁**
#### **1. 实现原理**
• **租约（Lease）机制**：客户端向 Etcd 申请租约（TTL），并将锁键值对与租约绑定。租约到期自动释放锁。
• **锁获取逻辑**：
  1. 客户端尝试写入带锁名称的键值对，若成功则获取锁。
  2. 未获取锁的客户端通过 `watch` 机制监听锁键变化。
  3. 锁释放后，客户端通过事务（`txn`）竞争获取锁。
• **锁续租**：客户端需定期续租以延长锁有效期，避免业务未完成时锁被自动释放。

#### **2. 优缺点**
• **优点**：
  • **高可用性**：基于 Raft 协议实现集群容错，无单点故障。
  • **高性能**：读写延迟低，适合高并发场景（如微服务配置中心）。
  • **灵活续租**：支持动态调整锁持有时间，减少锁竞争。
• **缺点**：
  • **实现复杂度高**：需处理租约续租和事务逻辑，代码复杂度较高。
  • **一致性代价**：Raft 协议的日志复制机制可能引入额外延迟。

---

### **对比总结**
|**维度​**​|​**​Zookeeper​**​|​**​Etcd​**​|​**​Redis (SETNX)​**​|
|---|---|---|---|
|​**​一致性协议​**​|ZAB 协议（强一致性）|Raft 协议（强一致性）|​**​最终一致性​**​（主从异步复制，可能短暂不一致）|
|​**​性能​**​|中低（节点操作需集群同步）|高（基于高效存储引擎）|​**​极高​**​（纯内存操作，单节点 QPS 可达 10万+）|
|​**​运维成本​**​|高（需维护多节点、选举机制）|中（自愈能力强，部署相对简单）|​**​低​**​（单节点即可运行，部署简单，但高可用需额外配置主从/集群）|
|​**​适用场景​**​|金融交易、强一致性要求的系统|云原生、高并发、微服务配置管理|​**​高并发短时任务​**​（如缓存扣减、限流、非关键资源锁定）|
|​**​学习成本​**​|较低（API 简单，但需理解 ZAB）|较高（需熟悉 Raft 和租约机制）|​**​低​**​（API 简单，但需处理锁续期、原子性等问题）|
### 场景分析

| **场景需求​**​      | ​**​推荐方案​**​  |
| --------------- | ------------- |
| 强一致性、长事务（如金融支付） | Zookeeper     |
| 高并发短任务、云原生环境    | Redis (SETNX) |
| 高可用、强一致性云原生配置管理 | Etcd          |
| 极致性能、简单场景（如缓存锁） | Redis (SETNX) |

---

### **实践建议**
1. **选择依据**：
   • 若系统对**强一致性**和**可靠性**要求极高（如支付系统），优先选择 Zookeeper。
   • 若需**高性能**和**云原生集成**（如 Kubernetes），Etcd 更优。
2. **优化方向**：
   • **Zookeeper**：通过临时顺序节点减少“羊群效应”，或结合异步监听优化性能。
   • **Etcd**：合理设置租约 TTL，避免频繁续租；利用 `txn` 事务简化锁竞争逻辑。

---

### **典型应用案例**
• **天翼云存储**：混合使用 Zookeeper 和 Etcd，根据业务场景选择锁机制，平衡性能与可靠性。
• **Kubernetes**：依赖 Etcd 实现资源调度锁，保障集群状态一致性。

通过合理选型与优化，Zookeeper 和 Etcd 均能有效解决分布式锁问题，开发者需结合系统需求权衡利弊。

