## 泛化能力和过拟合（欠拟合）
上一节讲到，我们的目标是为了让数据拟合的好（损失函数趋近于0），所以我们在训练数据时，会尽量命中让公式命中所有的点，但有时候，100%命中训练数据，也不一定就代表效果好。例如下图：
- 左边的简单线性模型，看似不如右边命中的点多，但实际上，右边是处于`过拟合`的状态，繁殖模型太简单无法捕获数据中的复杂模式，就称之为`欠拟合`。
- 过拟合的原因是由于模型太复杂了，把噪声和随机波动也学习会了，从而在新数据计算的表现能力上，效果反而不如简单的线性模型。
- 这种能够在没见过的数据上的表现能力，称之为`泛化能力`。

> 结论：模型不是越复杂越好

![image.png](https://happychan.oss-cn-shenzhen.aliyuncs.com/picgo/20250420210708.png)

那为什么复杂的模型会训练的不好，实际上是因为训练数据的缺失，我们增加足够的数据集，则复杂模型也能训练出繁`泛化能力`更好的模型。但有时候我们无法收集或者没必要收集更多的数据，还有一种方法就是`创造数据`，通过对原本数据的处理（例如对图片进行翻转、裁剪、增加噪声等），创造出更多的训练样本，这种方式就是`数据增强`，它能够让模型训练的数据更加贴近真实数据，减少随机波动和噪声导致模型结果波动（也就是鲁棒性）。

## 如何防止模型过拟合

### 提前终止训练

那么我们如何控制不让模型过拟合呢？我们可以通过抑制 w 和 b 参数的过渡增长，来解决，最简单的方法就是，提前终止训练过程，但是这种方式太粗糙了。

### 惩罚项和正则化

可以通过增加`惩罚项`来抑制参数的增长，将 w 加入损失函数的计算，每当 w 增加时，原损失函数减小，而新的目标函数增大。当w调大后，如果让损失函数减少的并不多，导致新的目标函数反而更大时，就认为本次调整并不合适，从而抑制参数的野蛮增长，这种往损失函数中添加权重惩罚项的方法，称之为`正则化`。
> 正则化的作用 : 让机器保证正确率的同时，训练出小的W系数

为什么需要小的系数，当系数很大时，随意的数据波动就会造成巨大的结果偏差，而小的系数，其泛化能力则更强，不会因为部分异常而导致结果严重误差，例如下面两个模型：
- 模型(1) : y1 = 0.3 * 8+0.4 * 8 + 0.5 = 22.9
- 模型(2) : y2 = 3 * 8+ 4 * 8 + 5 = 229
比较y1 和 y2可以发现，现实样本的一点偏差就导致了预测结果的巨大偏差

![image.png](https://happychan.oss-cn-shenzhen.aliyuncs.com/picgo/20250420212508.png)

- 正则化系数：λ ，用于控制惩罚项变化率
- 求绝对值的方式，称为 L1 正则化
- 求平台和的方式，称为 L2 正则化
- 不同的 λ  称为 L1 范数和 L2 范数
- 所有的这些正则化系数，统称为 超参数

![image.png](https://happychan.oss-cn-shenzhen.aliyuncs.com/picgo/20250420213845.png)

### Dropout
通过随机丢弃部分数据，来增强模型训练效果，从而适应更多的场景，这种方法很简单，但就是很有效果

## 其他问题
不做深入讨论

![image.png](https://happychan.oss-cn-shenzhen.aliyuncs.com/picgo/20250420214658.png)

