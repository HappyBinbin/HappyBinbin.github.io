## 损失函数
上一节讲到，我们需要计算出神经网络中的w 和 b，不断进行拟合，找到最接近真实数据的结果。那什么样的 w 和 b 才算是好的结果呢，当然是越接近结果的答案越好；因此就衍生出一个评判结果好坏的标准，也就是`损失函数`，代表了真实值和预测值的误差。
$$
\sum _{i=1}^N |y_{1}-\hat{y}_{i}|
$$
例如最简单的线性计算，只需要计算每个点位与直线在y轴上的距离差的绝对值即可；
![image.png](https://happychan.oss-cn-shenzhen.aliyuncs.com/picgo/20250420122917.png)

## 均方误差 MSE
绝对值在计算中不太好表示，所以可以改变一下公式的表达方式，利用数学计算来替换绝对值算法，例如：通过平方，再除以样本的数量
- 将 L 认为是损失函数，它就是一个关于 w 和 b 的函数

$$
L(w,b) = \frac{1}{N}\sum _{i=1}^N (y_{1}-\hat{y}_{i})^2
$$
## 回归目的
上述讲了各种概念，这里再次确定一下我们的目的是什么
$$
\begin{aligned}
数据&：(x_{1},y_{1})(x_{2},y_{2})(x_{3},y_{3}) \\
线性模型&：y = wx + b \\
损失函数&：L(w,b) = \frac{1}{N}\sum _{i=1}^N (y_{1}-\hat{y}_{i})^2 \\
目的&：求解让 L 最小的 w 和 b 值

\end{aligned}
$$
如何找到最小值，对于单元函数，例如我们先忽略 b ，只考虑 w，用最简单的线性模型数据举例：
$$
\begin{aligned}
数据&：(1,1) (2,2) (3,3) (4,4)\\
线性模型&：y = wx \\
损失函数展开&：\\
第一步，替换 \hat{y}&： L(w,b) = \frac{1}{N}\sum _{i=1}^N (y_{1}-wx_{i}) \\
第二步，展开求和&：L(w,b) = \frac{1}{4}[(y_{1}-wx_{1})^2 + (y_{2}-wx_{2})^2 + (y_{3}-wx_{3})^2 + (y_{4}-wx_{4})^2] \\
第三步，带入数据，计算结果&：L(w,b) = 7.5 + 15w + 7.5w^2 \\
第四步&：对 w 求导，让导数等于0，得到 w = 1
\end{aligned}
$$
如过加上 b 这个参数，求值就需要 `偏导数`：

> ​**​偏导数​**​是多元函数中对其中一个变量的导数，其他变量视为常数。它衡量当仅改变某一变量时，函数值的变化率。


而上述这种通过寻找线性函数来拟合 x 和 y 之间的关系等方法，就被称之为 `线性回归` 。

对于更加非线性函数，往往不能直接通过让导数或者偏导数为0，直接计算出最小值，所以目前的方法就是一点点地尝试，不断调整w和b的值，来让损失函数L接近最小。

w 或者 b 的变化大小，使得 L 变化的大小，分别就是损失函数对 w 和 b 的偏导数。
$$
\frac{∂L}{∂w} , \frac{∂L}{∂b}
$$
因此，如果我们要调整 w 和 b ，让 L 变小，则需要转换一下我们的计算目的，例如：让 w 和 b 不断往偏导数的反方向去变化，再通过一个参数 η 控制变化的速率（学习率），这个结果形成的向量，就被称为`梯度`。
$$
\begin{aligned}
w = w - η * \frac{∂L}{∂w} \\
b = b - η * \frac{∂L}{∂w}
\end{aligned}
$$

> 梯度是函数在某一点上所有方向上的变化率最大值，方向指向函数增长最快的方向。梯度下降则利用梯度的反方向（即下降最快方向）调整参数，使损失函数值逐步减小。

如何理解`梯度`，可类比为“圆球从山顶滚向山脚”的过程：
- ​​初始位置​​：随机初始化参数（如山顶某点）；
- ​​找坡度​​：计算梯度确定最陡下降方向；
- ​​控制步长​​：学习率决定每次滚动的距离；
- ​​迭代优化​​：重复调整方向直至梯度接近零（到达局部最低点）

这刚好与我们的目的（找到最小的损失函数值）相同。
上述的线性回归例子中，我们的偏导数就是一个简单的一元二次函数求导，而在非线性函数中，其本身可能就是一个复杂到bt的公式，求导就更加复杂了。那么，应该如何计算偏导数呢？
通过计算每一层的偏导数结果，然后相乘，就能得到最终的偏导数结果，这种方法也被称为 `链式法则`。
![image.png](https://happychan.oss-cn-shenzhen.aliyuncs.com/picgo/20250420142417.png)

我们前面提到，根据输入到 x 到值，计算出 y 结果 的过程，被称之为 `前向传播`。
我们又可以通过损失函数 ，从右向左依次求导，计算出损失函数L关于每个参数的梯度，前一层的值在后一层也会用到，这个计算过程就称之为 `反向传播`，这样一次前向传播和反向传播的过程，就是一次神经网络的训练。