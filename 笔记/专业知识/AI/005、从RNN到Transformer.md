
## 词编码

给你一句话"他开心我就生气"，让你判断里面每个词的含义；或者给你一句话，让你推断下一个字是什么；可以把这些过程设计成一个神经网络函数，来实现上述功能。
但这些“他”、“开心”... 等词语，计算机识别不了，所以我们要对这些词进行编码；

### 纯数字标识
也就是一个词用一个数字标识来表示，输入的词表有多大，数字标识就有多大；

缺点：
- 维度太低了，一个数字标识就是一维的向量
- 数字标识本身对语言理解没有任何意义，无法衡量词与词之间的相关性
### one-hot
准备一个超级大的向量，每个词在向量中占据一个位置为1
缺点：
- 维度太高，且非常系数
- 如果词表大小为10w，则每个向量都有10w个维度，每个向量都是正交的，词与词之间仍然没有相关性

![image.png](https://happychan.oss-cn-shenzhen.aliyuncs.com/picgo/20250422221245.png)

### 词嵌入

上述词编码的两种方式，都无法很好调和词维度与词关系的平衡。所以引入了一个新的方法，叫做`词嵌入`。它是通过训练得出来的结果，而不是人为定义的，将自然语言的联系，转化为可以通过数据公式计算的方式。例如：
- 每个向量中的值可以代表一些特征
- 数学上通过 `余弦相似度`和 `点积`两种方式计算出来的，感兴趣可以了解一下 [word2vec](https://github.com/dav/word2vec)
- 不同向量之间的加减乘除是相等的
![image.png](https://happychan.oss-cn-shenzhen.aliyuncs.com/picgo/20250422221919.png)

把所有的词向量组合起来就是一个`嵌入矩阵`
- 纵向代表词向量的维度
- 横向代表词表的大小
- 某一列就是一个词向量
![image.png](https://happychan.oss-cn-shenzhen.aliyuncs.com/picgo/20250422222147.png)

缺点：
- 虽然词向量的维度并不高，例如只有300个维度，但是不同的词组合成的句子，作为神经网络中的输入层，就会导致输入层的太大了
- 输入层的神经元数量会随则一句话中的词语数量而变化，是变长的
- 词与词之间的先后顺序无法体现，例如，“他” 在 “开心” 之前

## RNN

那么有什么方法能解决上面的问题呢？能让词与词之间知道顺序关系，并且减少输入层的大小。
继续往下看，我们针对每个词的神经元计算公式为：
$$
\begin{aligned}
他：x^1 \to g(WX^1 + b) \to Y_{1} \\
开心：x^2 \to g(WX^2 + b) \to Y_{2} \\
\end{aligned}
$$
每个词的计算过程是独立的，那如何让他们关联起来呢，显而易见，就是让前面的词，参与后面的词的计算过程即可：
- $h^{<i>}$ 就是隐藏状态
- $W_{xh}$：针对词向量的矩阵
- $W_{hh}$：针对隐藏状态的矩阵
- $W_{hy}$：针对输出结果的矩阵
- 对于偏置项 b 也是一样，有不同的偏执项

![image.png](https://happychan.oss-cn-shenzhen.aliyuncs.com/picgo/20250422223923.png)

简化后的公式就是

![image.png](https://happychan.oss-cn-shenzhen.aliyuncs.com/picgo/20250422224156.png)

缺点：
- 信息会随着时间步的增加而丢失，而某些语句，恰恰是首尾才有关系
- RNN的计算有顺序性，无法利用计算的的并行计算能力
