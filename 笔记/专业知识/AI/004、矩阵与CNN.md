## 矩阵运算
对于多个神经元输入层和输出层，对应的公式为：
$$
\begin{aligned}
y_{1} &= g(w_{11}x_{1} + w_{12} + x_{2}+w_{13}+x_{3}+b_{1}) \\
y_{2} &= g(w_{21}x_{1} + w_{22} + x_{2}+w_{23}+x_{3}+b_{2}) \\ 
\end{aligned}
$$
但是这样写下去公式太麻烦了，所以人们用矩阵运算来简化公式（省略g）：
$$
\begin{bmatrix} y_1 \\ y_2 \end{bmatrix} = 
\begin{bmatrix} w_{11} & w_{12} & w_{13} \\ w_{21} & w_{22} & w_{23} \end{bmatrix}
\begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}
+ 
\begin{bmatrix} b_1 \\ b_2 \end{bmatrix}
$$
看起来还是有点复杂，可以再简化一下：
$$
\begin{bmatrix} Y \end{bmatrix} = 
\begin{bmatrix} W \end{bmatrix}
\begin{bmatrix} X \end{bmatrix} + 
\begin{bmatrix} b \end{bmatrix}
$$
再简化一下：
$$
Y = g(WX + b)
$$

再简化一下，把所有每一层都抽象为$a^{[i]}$，就是下面这样：
- 每一层神经元的值都是上一层的函数
- 把麻烦的加减乘除，替换为简单的矩阵运算，可以加快并行运算的速度
![image.png](https://happychan.oss-cn-shenzhen.aliyuncs.com/picgo/20250421214100.png)

### CNN
根据上面的图可以看到，每一层的神经元都和前一层的神经元完全相连，这种结构就是`全连接层`。假设我们的输入时一张30x30的图片，则平铺展开后，输入层的神经元就达到了900个，如果下一层的神经元是1000个，那么全连接层参数数量就是90w个。
- 参数数量太大了，计算不过来
- 这种方式无法很好保存像素之间的空间关系，如果图片移动一下或者变暗一下，神经元的输入就会完全不同
计算机视觉中的做法：
- 通过卷积核，进行卷积运算，来提取图片的特征，例如：锐化、模糊、浮雕、轮廓等
- 可以保留图片原本的空间关系

在深度学习领域，其卷积核的值就是未知的，也是训练出来的一组值，我们的目的就是将神经网络中的全连接层，替换为卷积层，来减少训练的参数数量，也能更有效地捕捉参数的特征。
- 卷积层
- 池化层：对卷积层后的特征进行降维，减少计算量，保留主要特征
所有的这些层结合起来，就是卷积神经网络，CNN
![image.png](https://happychan.oss-cn-shenzhen.aliyuncs.com/picgo/20250421215111.png)



